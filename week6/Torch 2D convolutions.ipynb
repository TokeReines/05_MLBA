{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structured-tuning",
   "metadata": {
    "id": "structured-tuning"
   },
   "source": [
    "# PyTorch 2D convolutions\n",
    "#### Christian Igel, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ooBCszdvpKZs",
   "metadata": {
    "id": "ooBCszdvpKZs"
   },
   "source": [
    "## Mount driver and establish workspace\n",
    "First part is only of interest if you are using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Q6YBvOQDpVg3",
   "metadata": {
    "id": "Q6YBvOQDpVg3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "F3LgNo85paJH",
   "metadata": {
    "id": "F3LgNo85paJH"
   },
   "outputs": [],
   "source": [
    "## go to your folder\n",
    "#os.chdir('$Your Space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "realistic-banana",
   "metadata": {
    "id": "realistic-banana"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-craft",
   "metadata": {
    "id": "sized-craft"
   },
   "source": [
    "## One input channel, one output, no padding\n",
    "Let's define a `W`$\\times$`W` filter. For the following examples, we do not need a bias parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-granny",
   "metadata": {
    "id": "capital-granny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We just defined: Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Convolution filter is of size W\n",
    "W = 3\n",
    "# 1 input (image) channel, 1 output channel, WxW convolution kernel\n",
    "conv = nn.Conv2d(1, 1, W, bias=False)\n",
    "print(\"We just defined:\", conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-throat",
   "metadata": {
    "id": "virgin-throat"
   },
   "source": [
    "Let's look at the kernel dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "homeless-discovery",
   "metadata": {
    "id": "homeless-discovery",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1 output channel, 1 input channel, 1st dimension = W, 2nd dimension = W\n",
    "print(conv.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-working",
   "metadata": {
    "id": "ordered-working"
   },
   "source": [
    "The filter parameters are initialized randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excess-pennsylvania",
   "metadata": {
    "id": "excess-pennsylvania"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0338, -0.0080, -0.1272],\n",
      "          [ 0.1881,  0.1227, -0.0648],\n",
      "          [ 0.2708, -0.2209, -0.2681]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-monaco",
   "metadata": {
    "id": "lonely-monaco"
   },
   "source": [
    "We can set the parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ambient-sender",
   "metadata": {
    "id": "ambient-sender"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-evolution",
   "metadata": {
    "id": "serial-evolution"
   },
   "source": [
    "Let's define an input (image) `x`. The input is of the same shape as the filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preceding-spank",
   "metadata": {
    "id": "preceding-spank",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "Sum of all input elements: 36.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(float(W*W))\n",
    "x = torch.reshape(x, (1, 1, W, W))\n",
    "print('Input:\\n', x)\n",
    "print('Sum of all input elements:', torch.sum(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-benjamin",
   "metadata": {
    "id": "streaming-benjamin"
   },
   "source": [
    "Because there is no padding and input and filter have the same size, there is only one valid position for the filter. Accordingly, the result is a tensor with a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "million-bunch",
   "metadata": {
    "id": "million-bunch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([[[[36.]]]], grad_fn=<ConvolutionBackward0>) scalar: 36.0\n"
     ]
    }
   ],
   "source": [
    "c = conv(x)\n",
    "print('Tensor:', c, 'scalar:', c.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-positive",
   "metadata": {
    "id": "final-positive"
   },
   "source": [
    "The scalar should be equal to the sum of all input elements (ensure that you understand why)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-stomach",
   "metadata": {
    "id": "limiting-stomach"
   },
   "source": [
    "## One input channel, one output,  padding\n",
    "Now we add zero-padding such that the input dimensionality is preseved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "naughty-alfred",
   "metadata": {
    "id": "naughty-alfred"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 8., 15., 12.],\n",
      "          [21., 36., 27.],\n",
      "          [20., 33., 24.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(1, 1, W, padding=W//2, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-communication",
   "metadata": {
    "id": "invalid-communication"
   },
   "source": [
    "## Several input channels, one output, no padding\n",
    "Typically, the input to a convolutional layer consists of several feature maps or channels. For example, consider a 2D input with three channels (e.g., an RGB colour image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amber-somerset",
   "metadata": {
    "id": "amber-somerset"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.]],\n",
      "\n",
      "         [[ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]],\n",
      "\n",
      "         [[18., 19., 20.],\n",
      "          [21., 22., 23.],\n",
      "          [24., 25., 26.]]]])\n",
      "Sum of all inputs: 351.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(float(3*W*W))\n",
    "x = torch.reshape(x, (1, 3, W, W))\n",
    "print('Input:', x)\n",
    "print('Sum of all inputs:', torch.sum(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-porter",
   "metadata": {
    "id": "voluntary-porter"
   },
   "source": [
    "Let's define a convolutional layer that takes three channels as input and produces a single output feature map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spare-ranch",
   "metadata": {
    "id": "spare-ranch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight parameters of convolutional layer: Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 3 input (image) channels, 1 output channel, WxW convolution kernel\n",
    "conv = nn.Conv2d(3, 1, W, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print('Weight parameters of convolutional layer:', conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-wrapping",
   "metadata": {
    "id": "prime-wrapping"
   },
   "source": [
    "Note that there is one filter for each input channel.\n",
    "The convolutional layer first convolves each input channel with the corresponding filter.\n",
    "This results in three feature maps, whih are added to give the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "remarkable-prayer",
   "metadata": {
    "id": "remarkable-prayer",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of filter parameters: 27 \n",
      "result of filtering the input: tensor([[[[351.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = conv(x)\n",
    "print('number of filter parameters:', conv.weight.numel(), '\\nresult of filtering the input:', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-calcium",
   "metadata": {
    "id": "accessible-calcium"
   },
   "source": [
    "It is important that the number of parameters and the dimesionality of the result is clear to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-antarctica",
   "metadata": {
    "id": "expected-antarctica"
   },
   "source": [
    "Now let's apply 1$\\times$1 convolutions to our three input channels. Again, we set all filter weights to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "likely-front",
   "metadata": {
    "id": "likely-front",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 3 input (image) channels, 1 output channel, 1x1 convolution kernel\n",
    "conv = nn.Conv2d(3, 1, 1, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-resort",
   "metadata": {
    "id": "advisory-resort"
   },
   "source": [
    "This convolutional layer adds the three input feature maps/channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amateur-sharing",
   "metadata": {
    "id": "amateur-sharing",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[27., 30., 33.],\n",
      "          [36., 39., 42.],\n",
      "          [45., 48., 51.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-liberty",
   "metadata": {
    "id": "million-liberty"
   },
   "source": [
    "Thus, 1$\\times$1 convolutions can be used to compute weighted sums of input feature maps/channels (in our previous example, all weights were set to 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-rolling",
   "metadata": {
    "id": "threatened-rolling"
   },
   "source": [
    "## Several output maps\n",
    "Typically, convolutional layer produce several feature maps or channels. For example, consider \n",
    "extending the previous 1$\\times$1 example to two output maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effective-reform",
   "metadata": {
    "id": "effective-reform",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]],\n",
      "\n",
      "\n",
      "        [[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 3 input (image) channels, 2 output channel, 1x1 convolution kernel\n",
    "conv = nn.Conv2d(3, 2, 1, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "print(conv.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-while",
   "metadata": {
    "id": "veterinary-while"
   },
   "source": [
    "This layer maps 3 input feature maps to 2 output feature maps, which are identical in our example, because we initialized all filters so that they are identical: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "civilian-dallas",
   "metadata": {
    "id": "civilian-dallas",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[27., 30., 33.],\n",
      "          [36., 39., 42.],\n",
      "          [45., 48., 51.]],\n",
      "\n",
      "         [[27., 30., 33.],\n",
      "          [36., 39., 42.],\n",
      "          [45., 48., 51.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-arrow",
   "metadata": {
    "id": "korean-arrow"
   },
   "source": [
    "The first convolutional layer in a network has typically more output feature maps than input channels. Let's assume 3 input channels, 4 output channels of the same dimensionality (i.e., we use padding), and a filter size of 3. For each output channel, we have 3 filter with 9 parameters/weights each. Thus, we have 108 parameters in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infrared-renewal",
   "metadata": {
    "id": "infrared-renewal",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1301, -0.0715, -0.1222],\n",
      "          [ 0.0221,  0.1702, -0.1153],\n",
      "          [ 0.0441, -0.1402, -0.1661]],\n",
      "\n",
      "         [[ 0.0935,  0.1024,  0.0565],\n",
      "          [-0.0417, -0.1510, -0.1292],\n",
      "          [-0.1133, -0.0045,  0.0033]],\n",
      "\n",
      "         [[ 0.1514,  0.1489, -0.0205],\n",
      "          [ 0.1256, -0.0183,  0.0098],\n",
      "          [ 0.1078, -0.0516, -0.1825]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0376,  0.0628,  0.1075],\n",
      "          [-0.1282,  0.1426,  0.0187],\n",
      "          [ 0.1654,  0.1250,  0.1421]],\n",
      "\n",
      "         [[ 0.1029, -0.0857, -0.1679],\n",
      "          [-0.0849,  0.1593, -0.0509],\n",
      "          [-0.0989,  0.1331,  0.0343]],\n",
      "\n",
      "         [[ 0.0652, -0.0789,  0.1101],\n",
      "          [ 0.0765,  0.0385, -0.0329],\n",
      "          [-0.1301,  0.0496,  0.1360]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0960,  0.0977, -0.0544],\n",
      "          [ 0.0474,  0.1828,  0.0178],\n",
      "          [ 0.0317,  0.0563,  0.0984]],\n",
      "\n",
      "         [[ 0.1120, -0.0777,  0.0216],\n",
      "          [ 0.0924, -0.1714, -0.0334],\n",
      "          [ 0.0091,  0.0666,  0.0263]],\n",
      "\n",
      "         [[ 0.0765, -0.1113, -0.1785],\n",
      "          [ 0.1361,  0.0933, -0.1601],\n",
      "          [ 0.1070, -0.1483,  0.0623]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0867, -0.1529,  0.0373],\n",
      "          [ 0.0912, -0.0483,  0.0944],\n",
      "          [-0.1238, -0.0813, -0.0841]],\n",
      "\n",
      "         [[-0.1465,  0.1378,  0.1685],\n",
      "          [ 0.1897, -0.1570,  0.1086],\n",
      "          [-0.1266,  0.1341,  0.1479]],\n",
      "\n",
      "         [[ 0.0138,  0.1008, -0.1890],\n",
      "          [ 0.0335, -0.0902, -0.0377],\n",
      "          [ 0.1296, -0.0732, -0.0516]]]], requires_grad=True)\n",
      "Number of parameters: 108\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 4, W, padding=W//2, bias=False)\n",
    "print(conv.weight)\n",
    "print(\"Number of parameters:\", conv.weight.shape.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-chair",
   "metadata": {
    "id": "primary-chair"
   },
   "source": [
    "And here are the resulting feature maps when applied to our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "blond-damages",
   "metadata": {
    "id": "blond-damages"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -9.1031,  -6.9547,  -0.5691],\n",
      "          [ -7.7737,  -1.5306,   7.1818],\n",
      "          [ -0.3736,   6.7007,  10.5155]],\n",
      "\n",
      "         [[  8.0312,   6.1163,   3.4246],\n",
      "          [  9.0455,   8.5774,   4.6769],\n",
      "          [  0.8348,   3.5675,   4.7248]],\n",
      "\n",
      "         [[ -3.2627,   2.5606,   4.3049],\n",
      "          [ -8.9245,   0.2524,   5.5971],\n",
      "          [-10.4893,  -2.6879,   6.5485]],\n",
      "\n",
      "         [[ -2.2950,   0.7086,  -0.5059],\n",
      "          [ -1.5237,   1.0943,   1.2387],\n",
      "          [ -1.8586,   1.1034,   1.3416]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = conv(x)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-ozone",
   "metadata": {
    "id": "central-ozone"
   },
   "source": [
    "# Image processing examples\n",
    "Now we consider a more complex example that involves some basic image transformations. First, we need to import NumPy and some image utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "preliminary-discount",
   "metadata": {
    "id": "preliminary-discount"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-demographic",
   "metadata": {
    "id": "concerned-demographic"
   },
   "source": [
    "Let's load an image and convert it to grayscale so that we just deal with a single channel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "surprised-advancement",
   "metadata": {
    "id": "surprised-advancement"
   },
   "outputs": [],
   "source": [
    "image = Image.open('diku.jpg')  # Load image\n",
    "image = torchvision.transforms.functional.to_grayscale(image)  # Transform to grayscale, because we only wnat one channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-netscape",
   "metadata": {
    "id": "colonial-netscape"
   },
   "source": [
    "Let's plot the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "median-coordinate",
   "metadata": {
    "id": "median-coordinate"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL image shape: (42, 134) min: 0 max: 255\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_np = np.asarray(image) \n",
    "print(\"PIL image shape:\", img_np.shape, \"min:\", img_np.min(), \"max:\", img_np.max())\n",
    "plt.imshow(image, cmap='gray', vmin=0, vmax=255);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-lingerie",
   "metadata": {
    "id": "broadband-lingerie"
   },
   "source": [
    "The transformation of the image to a tensor maps has two important effects. First, the values are rescaled to $[0.,1.]$. Second, the channels become the first dimension.  The latter implies that, if we want to plot the image, we have to reorder the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caroline-strength",
   "metadata": {
    "id": "caroline-strength"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([1, 42, 134]) min: 0.0 max: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torchvision.transforms.ToTensor()(image)\n",
    "print(\"Tensor shape:\", x.shape, \"min:\", x.min().item(), \"max:\", x.max().item())\n",
    "plt.imshow(x.permute(1, 2, 0), cmap='gray', vmin=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-drill",
   "metadata": {
    "id": "suitable-drill"
   },
   "source": [
    "In order to be process by a layer, the tensor needs  another dimension/axis for enumerating the elements in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "heated-service",
   "metadata": {
    "id": "heated-service"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after adding batch dimension: torch.Size([1, 1, 42, 134])\n"
     ]
    }
   ],
   "source": [
    "x.unsqueeze_(0)  # Add a dimension\n",
    "print(\"Shape after adding batch dimension:\", x.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-rainbow",
   "metadata": {
    "id": "empirical-rainbow"
   },
   "source": [
    "Now we apply a simple horizontal gradient filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "trained-eligibility",
   "metadata": {
    "id": "trained-eligibility"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: tensor([[[[-1.,  1.]]]]) shape: torch.Size([1, 1, 1, 2])\n",
      "Tensor shape: torch.Size([1, 1, 42, 135]) min: -1.0 max: 1.0\n"
     ]
    }
   ],
   "source": [
    "hf = torch.tensor([[[[-1., 1.]]]])  # Define filter\n",
    "print(\"Kernel:\", hf, \"shape:\", hf.shape)\n",
    "\n",
    "conv = nn.Conv2d(1, 1, kernel_size=(1, 2), padding=(0, 1), bias=False)  # Padding only in one dimension needed\n",
    "conv.weight = torch.nn.Parameter(hf, requires_grad=False)  # Set kernel parameters to predefined filter parameters  \n",
    "c = conv(x)  # Apply filter\n",
    "print(\"Tensor shape:\", c.shape, \"min:\", c.min().item(), \"max:\", c.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-comedy",
   "metadata": {
    "id": "historic-comedy"
   },
   "source": [
    "We do not need a gradient for the kernel parameters, so we can use ``requires_grad=False``. This allows us to use ``c[0.0]`` as a NumPy array in the visualizaiton below. Alternatively, we could use ``c[0,0].detach()`` in the ``imshow`` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "natural-trail",
   "metadata": {
    "id": "natural-trail",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 135])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(c[0,0].shape)\n",
    "plt.imshow(c[0,0], cmap='gray', vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c67111",
   "metadata": {},
   "source": [
    "Sobel filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa060366",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [1, 1, 3].  Tensor sizes: [3, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Toke\\Documents\\DIKU\\05_MLA\\week6\\Torch 2D convolutions.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sobel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     [\u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     [\u001b[39m2.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2.\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [\u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Toke/Documents/DIKU/05_MLA/week6/Torch%202D%20convolutions.ipynb#Y106sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m gx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(sobel)\u001b[39m.\u001b[39;49mexpand((\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [1, 1, 3].  Tensor sizes: [3, 3]"
     ]
    }
   ],
   "source": [
    "sobel = np.array([\n",
    "    [1., 0., -1.],\n",
    "    [2., 0., -2.],\n",
    "    [1., 0., -1.]\n",
    "    ])\n",
    "    \n",
    "gx = torch.tensor(sobel).expand((1, 1, 3))\n",
    "gx = torch.tensor([[[\n",
    "    [1., 0., -1.],\n",
    "    [2., 0., -2.],\n",
    "    [1., 0., -1.]\n",
    "    ]]]) \n",
    "gy = gx.transpose(2, 3)\n",
    "g = torch.sqrt(gx * gx + gy * gy)\n",
    "print(\"Sobel Kernel:\", g, \"shape:\", g.shape)\n",
    "\n",
    "#conv = nn.Conv2d(1, 1, kernel_size=(3, 3), padding=(1, 1), bias=False)  # Padding only in one dimension needed\n",
    "# 1 input (image) channels, 3 output channel, 3x3 convolution kernel\n",
    "conv = nn.Conv2d(1, 2, 3, bias=False)\n",
    "print(\"a\", torch.ones_like(conv.weight).shape)\n",
    "#conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight), requires_grad=False)\n",
    "conv.weight = torch.nn.Parameter(torch.cat((gx, gy, g),  dim=0), requires_grad=False)  # Set kernel parameters to predefined filter parameters  \n",
    "c = conv(x)  # Apply filter\n",
    "print(\"Image shape:\", c.shape, \"min:\", c.min().item(), \"max:\", c.max().item())\n",
    "plt.imshow(c[0,0], cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(c[0,1], cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(c[0,2], cmap='gray')\n",
    "plt.figure();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Torch 2D convolutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('04_MLA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0cd659e422badc289b09613c52a8a7df312b57a89fecb5e6f49cfc7970c8344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
